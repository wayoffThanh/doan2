#Cell 1 ‚Äì Import th∆∞ vi·ªán
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import category_encoders as ce
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import classification_report, confusion_matrix

nltk.download("stopwords")

#ƒê·ªçc d·ªØ li·ªáu
file_path = "/kaggle/input/daylalancuoihuhu/loan_data.xlsx"
df = pd.read_excel(file_path)

print("D·ªØ li·ªáu ƒë√£ load:", df.shape)
display(df.head())
#X√≥a c√°c d√≤ng ch·ª©a gi√° tr·ªã thi·∫øu
df = df.dropna()
print("Sau khi x√≥a NaN:", df.shape)

#T·∫°o c√°c ƒë·∫∑c tr∆∞ng logic
df["debt_to_income_ratio"] = df["total_outstanding_debt"] / (df["monthly_income"] + 1e-3)
df["expense_to_income"] = df["estimated_monthly_expense"] / (df["monthly_income"] + 1e-3)
df["is_unemployed"] = (df["employment_status"] == "Unemployed").astype(int)

#ƒêi·ªÉm r·ªßi ro k·∫øt h·ª£p
df["risk_score_raw"] = (
    df["expense_to_income"] * 0.4 +
    df["debt_to_income_ratio"] * 0.4 +
    (df["total_late_payments"] + df["num_late_payments_in_app"]) * 0.2
).round(3)

#S·ªë kho·∫£n vay t·ªïng
df["loan_count_score"] = df["number_of_current_loans"] + df["num_loans_from_app"]

#CBM Feature
df["cbm_feature_1"] = df["wallet_usage_frequency"] * df["bank_avg_balance"]
df["cbm_feature_2"] = df["risk_score_raw"] * df["loan_count_score"]

#Tr√≠ch t·ª´ kh√≥a b·∫±ng TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

corpus = df["loan_purpose_text"].astype(str).tolist()
vectorizer = TfidfVectorizer(max_features=1000, stop_words=stopwords.words("english"))
X_text = vectorizer.fit_transform(corpus)

feature_names = np.array(vectorizer.get_feature_names_out())
top_keywords = []

for row in X_text:
    indices = np.argsort(row.toarray()).flatten()[::-1][:3]
    keywords = feature_names[indices]
    top_keywords.append(", ".join(keywords))

df["loan_keywords"] = top_keywords
#Ki·ªÉm tra c√°c c·ªôt ƒë√£ t·∫°o
display(df[[
    "debt_to_income_ratio", "expense_to_income", "is_unemployed",
    "risk_score_raw", "loan_count_score", "cbm_feature_1", "cbm_feature_2", "loan_keywords"
]].head())
#Cell 3 ‚Äì Feature Engineering & Keyphrase Extraction
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

#X√≥a d√≤ng null, KH√îNG x√≥a d√≤ng c√≥ income = 0
df = df.dropna()

#T·∫°o ƒë·∫∑c tr∆∞ng ph·ª• tr·ª£
df["debt_to_income_ratio"] = df["total_outstanding_debt"] / (df["monthly_income"] + 1e-3)
df["expense_to_income"] = df["estimated_monthly_expense"] / (df["monthly_income"] + 1e-3)
df["is_unemployed"] = (df["employment_status"] == "Unemployed").astype(int)

#T·∫°o ƒëi·ªÉm r·ªßi ro th√¥ k·∫øt h·ª£p nhi·ªÅu y·∫øu t·ªë
df["risk_score_raw"] = (
    df["expense_to_income"] * 0.4
    + df["debt_to_income_ratio"] * 0.4
    + (df["total_late_payments"] + df["num_late_payments_in_app"]) * 0.2
).round(3)

#T·ªïng s·ªë kho·∫£n vay
df["loan_count_score"] = df["number_of_current_loans"] + df["num_loans_from_app"]

#T·∫°o ƒë·∫∑c tr∆∞ng trung gian CBM
df["cbm_feature_1"] = df["wallet_usage_frequency"] * df["bank_avg_balance"]
df["cbm_feature_2"] = df["risk_score_raw"] * df["loan_count_score"]

#Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ vƒÉn b·∫£n `loan_purpose_text`
corpus = df["loan_purpose_text"].astype(str).tolist()
vectorizer = TfidfVectorizer(max_features=1000, stop_words=stopwords.words("english"))
X_text = vectorizer.fit_transform(corpus)

feature_names = np.array(vectorizer.get_feature_names_out())
top_keywords = []

for row in X_text:
    indices = np.argsort(row.toarray()).flatten()[::-1][:3]
    keywords = feature_names[indices]
    top_keywords.append(", ".join(keywords))
df["loan_keywords"] = top_keywords

#Ki·ªÉm tra nhanh k·∫øt qu·∫£
df[["loan_purpose_text", "loan_keywords"]].head(10)

#Cell 4 ‚Äì Encode Tabular + Tokenize Text (FinBERT)

from sklearn.preprocessing import StandardScaler
import category_encoders as ce
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

#c·ªôt ƒë·∫ßu ra v√† c·ªôt vƒÉn b·∫£n
label_col = "default"
text_col = "loan_keywords"

#T·∫≠p c√°c ƒë·∫∑c tr∆∞ng tabular ƒë·∫ßu v√†o (bao g·ªìm c·∫£ CBM)
selected_features = [
    "age", "gender", "marital_status", "residential_area", "monthly_income", "estimated_monthly_expense",
    "employment_status", "job_type", "total_outstanding_debt", "number_of_current_loans", "total_late_payments",
    "num_loans_from_app", "num_late_payments_in_app", "has_bank_account_linked", "bank_avg_balance",
    "has_e_wallet_linked", "wallet_usage_frequency", 
    "cbm_feature_1", "cbm_feature_2"
]

# T√°ch d·ªØ li·ªáu tabular v√† text
X_tabular = df[selected_features].copy()
X_text = df[text_col].astype(str)
y = df[label_col]

# Encode c√°c c·ªôt d·∫°ng ph√¢n lo·∫°i
categorical_cols = X_tabular.select_dtypes(include="object").columns.tolist()
encoder = ce.OrdinalEncoder(cols=categorical_cols)
X_encoded = encoder.fit_transform(X_tabular)

#Scale ƒë·∫∑c tr∆∞ng s·ªë
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

#Tokenize vƒÉn b·∫£n b·∫±ng FinBERT
model_name = "yiyanghkust/finbert-tone"
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoded_text = tokenizer(
    X_text.tolist(),
    padding=True,
    truncation=True,
    max_length=32,
    return_tensors="pt"
)

# Bi·∫øn m·ª•c ti√™u
y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)

# üß™ Chia train/test
X_tab_train, X_tab_test, ids_train, ids_test, y_train, y_test, mask_train, mask_test = train_test_split(
    torch.tensor(X_scaled, dtype=torch.float32),
    encoded_text["input_ids"],
    y_tensor,
    encoded_text["attention_mask"],
    test_size=0.2,
    random_state=42
)

# T·∫°o Dataloader
train_dataset = TensorDataset(X_tab_train, ids_train, mask_train, y_train)
test_dataset = TensorDataset(X_tab_test, ids_test, mask_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

print("D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho m√¥ h√¨nh hu·∫•n luy·ªán.")
#Cell 5 ‚Äì Define v√† kh·ªüi t·∫°o m√¥ h√¨nh Hybrid TabTransformer + FinBERT + CBM

import torch.nn as nn
from transformers import AutoModel

# Khai b√°o device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TabFinBERT_CBModel(nn.Module):
    def __init__(self, tab_input_dim, text_model_name="yiyanghkust/finbert-tone", d_model=128, n_heads=4, dropout=0.1):
        super().__init__()

        self.tabular_proj = nn.Linear(tab_input_dim, d_model)
        self.tab_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True),
            num_layers=2
        )

        self.text_encoder = AutoModel.from_pretrained(text_model_name)
        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, d_model)

        self.cbm_proj = nn.Sequential(
            nn.Linear(2, d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        self.final = nn.Sequential(
            nn.Linear(d_model * 3, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x_tab, input_ids, attention_mask, cbm):
        tab_encoded = self.tabular_proj(x_tab)
        tab_out = self.tab_transformer(tab_encoded.unsqueeze(1)).squeeze(1)

        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = text_out.pooler_output
        text_out = self.text_proj(pooled)

        cbm_out = self.cbm_proj(cbm)
        combined = torch.cat([tab_out, text_out, cbm_out], dim=1)

        return self.final(combined)

#Kh·ªüi t·∫°o m√¥ h√¨nh
tab_input_dim = X_tab_train.shape[1]
model = TabFinBERT_CBModel(tab_input_dim=tab_input_dim).to(device)

print("M√¥ h√¨nh ƒë√£ kh·ªüi t·∫°o xong.")
#Cell 6 ‚Äì Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi BCEWithLogitsLoss + Early Stopping
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import roc_auc_score

# T√≠nh pos_weight cho BCEWithLogitsLoss
class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=df["default"])
pos_weight = torch.tensor([class_weights[1] / class_weights[0]]).to("cuda" if torch.cuda.is_available() else "cpu")

criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=4, factor=0.5, verbose=True)

# Early stopping config
best_loss = float('inf')
best_model_state = None
counter = 0
patience = 8

train_losses, test_losses = [], []
epochs = 50

for epoch in range(1, epochs + 1):
    model.train()
    total_train_loss = 0

    for x_tab, ids, mask, yb in train_loader:
        x_tab, ids, mask, yb = x_tab.to(device), ids.to(device), mask.to(device), yb.to(device)
        cbm = x_tab[:, -2:]  # CBM l√† 2 c·ªôt cu·ªëi trong tabular

        optimizer.zero_grad()
        logits = model(x_tab, ids, mask, cbm)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()

        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Validation
    model.eval()
    total_test_loss = 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for x_tab, ids, mask, yb in test_loader:
            x_tab, ids, mask, yb = x_tab.to(device), ids.to(device), mask.to(device), yb.to(device)
            cbm = x_tab[:, -2:]
            logits = model(x_tab, ids, mask, cbm)
            loss = criterion(logits, yb)
            total_test_loss += loss.item()

            preds = torch.sigmoid(logits)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(yb.cpu().numpy())

    avg_test_loss = total_test_loss / len(test_loader)
    test_losses.append(avg_test_loss)

    # AUC
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except:
        auc = -1

    print(f"poch {epoch:02d} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f} | AUC: {auc:.4f}")

    scheduler.step(avg_test_loss)

    if avg_test_loss < best_loss:
        best_loss = avg_test_loss
        best_model_state = model.state_dict()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered.")
            break

# Load best model
if best_model_state:
    model.load_state_dict(best_model_state)
    print("Loaded best model with lowest test loss.")
# Chuy·ªÉn x√°c su·∫•t th√†nh nh√£n nh·ªã ph√¢n v·ªõi ng∆∞·ª°ng 0.5
y_pred = np.array(all_preds).flatten()
y_true = np.array(all_labels).flatten()
y_pred_binary = (y_pred > 0.5).astype(int)

from sklearn.metrics import (
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,
    matthews_corrcoef, log_loss
)
import matplotlib.pyplot as plt

#Classification Report
print("Classification Report:\n")
print(classification_report(y_true, y_pred_binary, digits=4))

#Confusion Matrix
cm = confusion_matrix(y_true, y_pred_binary)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Default", "Default"])
disp.plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

#C√°c ch·ªâ s·ªë ƒë√°nh gi√° th√™m (t√πy ch·ªçn)
print("Extra Metrics:")
print("AUC Score:", roc_auc_score(y_true, y_pred))
print("Average Precision:", average_precision_score(y_true, y_pred))
print("Matthews Correlation Coefficient:", matthews_corrcoef(y_true, y_pred_binary))
print("Log Loss:", log_loss(y_true, y_pred))
plt.plot(train_losses, label="Train Loss")
plt.plot(test_losses, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss per Epoch")
plt.grid(True)
plt.show()
